{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cPickle\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "# keras imports\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.layers import Embedding, Dropout\n",
    "from keras.layers import Dense, Input, Flatten, ZeroPadding1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Merge, Dropout, Activation\n",
    "from keras.models import Model, Sequential\n",
    "\n",
    "from keras.engine import Layer, InputSpec\n",
    "import tensorflow as tf\n",
    "\n",
    "# sklearn imports\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "\n",
    "# allows for inline plotting\n",
    "%matplotlib inline\n",
    "\n",
    "# pretty plots\n",
    "plt.style.use(\"bmh\")\n",
    "\n",
    "# Globals\n",
    "MAX_SEQUENCE_LENGTH = 150\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Prepare Training data\"\"\"\n",
    "\n",
    "def get_reviews(labels_fname, genre, dirpath):\n",
    "\n",
    "    # read data\n",
    "    df_neg = pd.read_csv(labels_fname + '_neg_genres.csv', encoding='utf-8')\n",
    "    df_pos = pd.read_csv(labels_fname + '_pos_genres.csv', encoding='utf-8')\n",
    "    \n",
    "    # pull genre\n",
    "    genre_neg_idx = df_neg.loc[df_neg[genre] == 1]['ref'].values\n",
    "    genre_pos_idx = df_pos.loc[df_pos[genre] == 1]['ref'].values\n",
    "    \n",
    "    # pull titles\n",
    "    genre_neg_titles = df_neg.loc[df_neg[genre] == 1]['title'].values\n",
    "    genre_pos_titles = df_pos.loc[df_pos[genre] == 1]['title'].values\n",
    "    print len(genre_neg_idx), len(genre_pos_idx), len(genre_neg_titles), len(genre_pos_titles)\n",
    "\n",
    "\n",
    "    # get the review filenames\n",
    "    def get_fname(search_string):\n",
    "        return glob.glob(search_string)[0].split('/')[-1]\n",
    "\n",
    "    neg_dirpath = dirpath + \"neg/\"\n",
    "    genre_neg_fnames = [get_fname(neg_dirpath + str(ii) + '_*.txt') for ii in genre_neg_idx]\n",
    "\n",
    "    pos_dirpath = dirpath + \"pos/\"\n",
    "    genre_pos_fnames = [get_fname(pos_dirpath + str(ii) + '_*.txt') for ii in genre_pos_idx]\n",
    "\n",
    "    # pull the text of the reviews\n",
    "    def get_reviews(dirpath, fnames):\n",
    "        reviews = []\n",
    "        for fname in fnames:\n",
    "            with open(dirpath + fname, 'r') as myfile:\n",
    "                data=myfile.read()\n",
    "            reviews.append(data)\n",
    "        return reviews\n",
    "\n",
    "    genre_neg_reviews = get_reviews(neg_dirpath, genre_neg_fnames)\n",
    "    genre_pos_reviews = get_reviews(pos_dirpath, genre_pos_fnames)\n",
    "    genre_all_reviews = genre_neg_reviews + genre_pos_reviews\n",
    "    genre_all_titles = list(genre_neg_titles) + list(genre_pos_titles)\n",
    "    \n",
    "    df_out = pd.DataFrame(data=genre_all_reviews)\n",
    "    df_out.columns = ['text']\n",
    "    df_out['title'] = genre_all_titles\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5153 6589 5153 6589\n",
      "3785 4335 3785 4335\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dramatic license - some hate it, though it is ...</td>\n",
       "      <td>Lucy (TV Movie 2003)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This, along with \"Hare Tonic,\" ranks as one of...</td>\n",
       "      <td>Hare Conditioned (1945)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nothing's more enjoyable for me than a who-dun...</td>\n",
       "      <td>Deathtrap (1982)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I caught Evening in the cinema with a lady fri...</td>\n",
       "      <td>Evening (2007)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This movie has the most beautiful opening sequ...</td>\n",
       "      <td>A Matter of Life and Death (1946)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Dramatic license - some hate it, though it is ...   \n",
       "1  This, along with \"Hare Tonic,\" ranks as one of...   \n",
       "2  Nothing's more enjoyable for me than a who-dun...   \n",
       "3  I caught Evening in the cinema with a lady fri...   \n",
       "4  This movie has the most beautiful opening sequ...   \n",
       "\n",
       "                               title  class  \n",
       "0               Lucy (TV Movie 2003)      0  \n",
       "1            Hare Conditioned (1945)      1  \n",
       "2                   Deathtrap (1982)      1  \n",
       "3                     Evening (2007)      0  \n",
       "4  A Matter of Life and Death (1946)      0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pull drama reviews\n",
    "df_train_drama = get_reviews(\"stanford_train\",\n",
    "                             \"Drama\",\n",
    "                             \"/Users/dfinkel/Downloads/aclImdb/train/\")\n",
    "df_train_drama['class'] = 0\n",
    "\n",
    "# Pull comedy reviews\n",
    "df_train_comedy = get_reviews(\"stanford_train\",\n",
    "                              \"Comedy\",\n",
    "                              \"/Users/dfinkel/Downloads/aclImdb/train/\")\n",
    "df_train_comedy['class'] = 1\n",
    "\n",
    "# build training frame\n",
    "df_train = pd.concat([df_train_drama, df_train_comedy])\n",
    "\n",
    "# shuffle rows\n",
    "df_train = df_train.sample(frac=1).reset_index(drop=True)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19862\n",
      "17388\n"
     ]
    }
   ],
   "source": [
    "print len(df_train)\n",
    "df_train = df_train.drop_duplicates('text')\n",
    "print len(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/keras/preprocessing/text.py:172: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Tokenize the comments and \n",
    "convert to numerical sequences\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize comments\n",
    "texts = df_train['text'].values\n",
    "\n",
    "# Initialize tokenizer\n",
    "# nb_words tells tokenizer to only keep MAX_NB_WORDS\n",
    "# when texts_to_sequences is applied\n",
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "\n",
    "# encoding text tokens to sequences\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# convert text to numerical arrays\n",
    "sequences = tokenizer.texts_to_sequences(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 73556 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# count unique tokens in corpus\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Shape of data tensor:', (17388, 150))\n",
      "('Shape of label tensor:', (17388, 2))\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Bookkeeping\n",
    "\"\"\"\n",
    "# convert comment labels to categorical tensor\n",
    "labels = df_train['class']\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "\n",
    "# zero-pad comments\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive and negative reviews in traing and validation set \n",
      "[8374. 5537.]\n",
      "[2096. 1381.]\n"
     ]
    }
   ],
   "source": [
    "# randomly sort data into train/test sets\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]\n",
    "\n",
    "print('Number of positive and negative reviews in traing and validation set ')\n",
    "print y_train.sum(axis=0)\n",
    "print y_val.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 400000 word vectors in Glove 6B 100d.\n"
     ]
    }
   ],
   "source": [
    "# import GLOVE word embeddings\n",
    "# see: https://nlp.stanford.edu/projects/glove/\n",
    "GLOVE_DIR = \"/Users/dfinkel/proto_dev/data_science/glove_data/glove_files/\"\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Total %s word vectors in Glove 6B 100d.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create embedding layer \n",
    "using glove data\n",
    "\"\"\"\n",
    "\n",
    "# embedding matrix maps words onto vectors\n",
    "# initialize with uniform random numbers (?)\n",
    "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index \n",
    "        # will be uniform random numbers\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use model from https://arxiv.org/pdf/1404.2188.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMaxPooling(Layer):\n",
    "    \"\"\"\n",
    "    K-max pooling layer that extracts the k-highest activations from a sequence (2nd dimension).\n",
    "    TensorFlow backend.\n",
    "    \"\"\"\n",
    "    def __init__(self, k=1, axis=1, **kwargs):\n",
    "        super(KMaxPooling, self).__init__(**kwargs)\n",
    "        self.input_spec = InputSpec(ndim=3)\n",
    "        self.k = k\n",
    "\n",
    "        assert axis in [1,2],  'expected dimensions (samples, filters, convolved_values),\\\n",
    "                   cannot fold along samples dimension or axis not in list [1,2]'\n",
    "        self.axis = axis\n",
    "\n",
    "        # need to switch the axis with the last elemnet\n",
    "        # to perform transpose for tok k elements since top_k works in last axis\n",
    "        self.transpose_perm = [0,1,2] #default\n",
    "        self.transpose_perm[self.axis] = 2\n",
    "        self.transpose_perm[2] = self.axis\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        input_shape_list = list(input_shape)\n",
    "        input_shape_list[self.axis] = self.k\n",
    "        return tuple(input_shape_list)\n",
    "\n",
    "    def call(self, x):\n",
    "        # swap sequence dimension to get top k elements along axis=1\n",
    "        transposed_for_topk = tf.transpose(x, perm=self.transpose_perm)\n",
    "\n",
    "        # extract top_k, returns two tensors [values, indices]\n",
    "        top_k = tf.nn.top_k(transposed_for_topk, k=self.k, sorted=True, name=None)[0]\n",
    "\n",
    "        # return back to normal dimension but now sequence dimension has only k elements\n",
    "        # performing another transpose will get the tensor back to its original shape\n",
    "        # but will have k as its axis_1 size\n",
    "        transposed_back = tf.transpose(top_k, perm=self.transpose_perm)\n",
    "\n",
    "        return transposed_back\n",
    "\n",
    "\n",
    "class Folding(Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Folding, self).__init__(**kwargs)\n",
    "        self.input_spec = InputSpec(ndim=3)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[1], int(input_shape[2]/2))\n",
    "\n",
    "    def call(self, x):\n",
    "        input_shape = x.get_shape().as_list()\n",
    "\n",
    "        # split the tensor along dimension 2 into dimension_axis_size/2\n",
    "        # which will give us 2 tensors\n",
    "        splits = tf.split(x, num_or_size_splits=int(input_shape[2]/2), axis=2)\n",
    "\n",
    "        # reduce sums of the pair of rows we have split onto\n",
    "        reduce_sums = [tf.reduce_sum(split, axis=2) for split in splits]\n",
    "\n",
    "        # stack them up along the same axis we have reduced\n",
    "        row_reduced = tf.stack(reduce_sums, axis=2)\n",
    "        return row_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 150, 100)          7355700   \n",
      "_________________________________________________________________\n",
      "zero_padding1d_1 (ZeroPaddin (None, 248, 100)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 248, 64)           320064    \n",
      "_________________________________________________________________\n",
      "k_max_pooling_1 (KMaxPooling (None, 5, 64)             0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 5, 64)             0         \n",
      "_________________________________________________________________\n",
      "zero_padding1d_2 (ZeroPaddin (None, 53, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 53, 64)            102464    \n",
      "_________________________________________________________________\n",
      "folding_1 (Folding)          (None, 53, 32)            0         \n",
      "_________________________________________________________________\n",
      "k_max_pooling_2 (KMaxPooling (None, 5, 32)             0         \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 5, 32)             0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 322       \n",
      "=================================================================\n",
      "Total params: 7,778,550\n",
      "Trainable params: 7,778,550\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "model based on: https://arxiv.org/pdf/1404.2188.pdf\n",
    "\"\"\"\n",
    "\n",
    "# Initialize model\n",
    "model_1 = Sequential()\n",
    "\n",
    "# Add an embedding layer\n",
    "model_1.add(Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True))\n",
    "\n",
    "# zero bad 49 zeros to \n",
    "# both sides of comment\n",
    "model_1.add(ZeroPadding1D((49,49)))\n",
    "\n",
    "# 1D convolution\n",
    "# 64 channels in output\n",
    "# 50 values in convolution window\n",
    "# zero pad output to perserve size\n",
    "model_1.add(Conv1D(64, 50, padding=\"same\"))\n",
    "\n",
    "# Kmax pool\n",
    "# Return k max values\n",
    "# perserve order\n",
    "model_1.add(KMaxPooling(k=5, axis=1))\n",
    "\n",
    "# Activate w relu\n",
    "model_1.add(Activation(\"relu\"))\n",
    "\n",
    "# zero pad 24 zeros to\n",
    "# both sides of representation\n",
    "model_1.add(ZeroPadding1D((24,24)))\n",
    "\n",
    "# 1D convolution\n",
    "# 64 channels in output\n",
    "# 25 values in convolution window\n",
    "# zero pad output to perserve size\n",
    "model_1.add(Conv1D(64, 25, padding=\"same\"))\n",
    "\n",
    "# Fold output\n",
    "#   - sum 2 channels together\n",
    "#   - halves the channels\n",
    "model_1.add(Folding())\n",
    "\n",
    "# Kmax pool\n",
    "# Return k max values\n",
    "# perserve order\n",
    "model_1.add(KMaxPooling(k=5, axis=1))\n",
    "\n",
    "# Activate w relu\n",
    "model_1.add(Activation(\"relu\"))\n",
    "\n",
    "# Flatten the data\n",
    "model_1.add(Flatten())\n",
    "\n",
    "# Connect the neural net\n",
    "model_1.add(Dense(2, activation=\"softmax\"))\n",
    "\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/keras/models.py:981: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13911 samples, validate on 3477 samples\n",
      "Epoch 1/10\n",
      "13911/13911 [==============================] - 105s 8ms/step - loss: 0.6883 - acc: 0.6061 - val_loss: 0.6324 - val_acc: 0.6460\n",
      "Epoch 2/10\n",
      "13911/13911 [==============================] - 103s 7ms/step - loss: 0.5536 - acc: 0.7168 - val_loss: 0.5769 - val_acc: 0.7015\n",
      "Epoch 3/10\n",
      "13911/13911 [==============================] - 103s 7ms/step - loss: 0.4217 - acc: 0.8073 - val_loss: 0.5129 - val_acc: 0.7529\n",
      "Epoch 4/10\n",
      "13911/13911 [==============================] - 103s 7ms/step - loss: 0.2983 - acc: 0.8758 - val_loss: 0.5567 - val_acc: 0.7633\n",
      "Epoch 5/10\n",
      "13911/13911 [==============================] - 103s 7ms/step - loss: 0.1516 - acc: 0.9454 - val_loss: 0.6812 - val_acc: 0.7645\n",
      "Epoch 6/10\n",
      "13911/13911 [==============================] - 104s 7ms/step - loss: 0.0488 - acc: 0.9892 - val_loss: 0.8248 - val_acc: 0.7627\n",
      "Epoch 7/10\n",
      " 8832/13911 [==================>...........] - ETA: 35s - loss: 0.0108 - acc: 0.9988"
     ]
    }
   ],
   "source": [
    "model_1.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])\n",
    "\n",
    "model_1.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "          nb_epoch=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = df_train['text'].values\n",
    "Y_ = df_train['class'].values\n",
    "# train classifier\n",
    "# non tf-idf pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(ngram_range=(1, 3))),\n",
    "    ('clf', LogisticRegression(C=0.39810717055349731, class_weight=None)),])\n",
    "pipeline.fit(X_, Y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_informative_feature_for_binary_classification(pipeline, n=10):\n",
    "    class_labels = pipeline.named_steps['clf'].classes_\n",
    "    if 'feats' in pipeline.named_steps.keys():\n",
    "        feature_names = pipeline.named_steps['feats'].get_feature_names()\n",
    "    elif 'vect' in pipeline.named_steps.keys():\n",
    "        feature_names = pipeline.named_steps['vect'].get_feature_names()\n",
    "    topn_class1 = sorted(zip(pipeline.named_steps['clf'].coef_[0], feature_names))[:n]\n",
    "    topn_class2 = sorted(zip(pipeline.named_steps['clf'].coef_[0], feature_names))[-n:]\n",
    "\n",
    "#     for coef, feat in topn_class1:\n",
    "#         print class_labels[0], coef, feat\n",
    "\n",
    "#     print\n",
    "\n",
    "#     for coef, feat in reversed(topn_class2):\n",
    "#         print class_labels[1], coef, feat\n",
    "    return topn_class1, topn_class2\n",
    "\n",
    "def extract_features(pipeline):\n",
    "    \"\"\"\n",
    "    extract the informative features\n",
    "    from the classifier\n",
    "    \"\"\"\n",
    "    # vect = pipeline.steps[0][1]\n",
    "    # clf = pipeline.steps[2][1]\n",
    "    topn_class1, topn_class2 = most_informative_feature_for_binary_classification(pipeline, n=200000)\n",
    "\n",
    "    class1 = pd.DataFrame(data=topn_class1, columns=(('weight', 'word')))\n",
    "    class2 = pd.DataFrame(data=topn_class2, columns=(('weight', 'word')))\n",
    "\n",
    "    class1['ngram_size'] = class1['word'].apply(meas_length)\n",
    "    class2['ngram_size'] = class2['word'].apply(meas_length)\n",
    "\n",
    "    # name columns\n",
    "    class1.columns = ['Weight', 'word', 'ngram_size']\n",
    "    class2.columns = ['Weight', 'word', 'ngram_size']\n",
    "\n",
    "    # break out unigrams, bigrams and trigrams\n",
    "    unigrams = pd.concat([class1.loc[class1['ngram_size'] == 1], class2.loc[class2['ngram_size'] == 1]])\n",
    "    bigrams = pd.concat([class1.loc[class1['ngram_size'] == 2], class2.loc[class2['ngram_size'] == 2]])\n",
    "    trigrams = pd.concat([class1.loc[class1['ngram_size'] == 3], class2.loc[class2['ngram_size'] == 3]])\n",
    "    return unigrams, bigrams, trigrams\n",
    "\n",
    "# Extract the unigrams, bigrams and trigrams\n",
    "# used by the trained classifier\n",
    "unigrams, bigrams, trigrams = extract_features(pipeline)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
