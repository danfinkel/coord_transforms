{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify Movies as Comedy or Drama\n",
    "### Dan Finkel\n",
    "### September 7 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cPickle\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "# keras imports\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.layers import Embedding, Dropout\n",
    "from keras.layers import Dense, Input, Flatten, ZeroPadding1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Merge, Dropout, Activation\n",
    "from keras.models import Model, Sequential\n",
    "\n",
    "from keras.engine import Layer, InputSpec\n",
    "import tensorflow as tf\n",
    "\n",
    "# sklearn imports\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "\n",
    "# allows for inline plotting\n",
    "%matplotlib inline\n",
    "\n",
    "# pretty plots\n",
    "plt.style.use(\"bmh\")\n",
    "\n",
    "# Globals\n",
    "MAX_SEQUENCE_LENGTH = 150\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Prepare Training data\"\"\"\n",
    "\n",
    "def get_reviews(labels_fname, genre, dirpath):\n",
    "\n",
    "    # read data\n",
    "    df_neg = pd.read_csv(labels_fname + '_neg_genres.csv', encoding='utf-8')\n",
    "    df_pos = pd.read_csv(labels_fname + '_pos_genres.csv', encoding='utf-8')\n",
    "    \n",
    "    # pull genre\n",
    "    genre_neg_idx = df_neg.loc[df_neg[genre] == 1]['ref'].values\n",
    "    genre_pos_idx = df_pos.loc[df_pos[genre] == 1]['ref'].values\n",
    "\n",
    "    # get the review filenames\n",
    "    def get_fname(search_string):\n",
    "        return glob.glob(search_string)[0].split('/')[-1]\n",
    "\n",
    "    neg_dirpath = dirpath + \"neg/\"\n",
    "    genre_neg_fnames = [get_fname(neg_dirpath + str(ii) + '_*.txt') for ii in genre_neg_idx]\n",
    "\n",
    "    pos_dirpath = dirpath + \"pos/\"\n",
    "    genre_pos_fnames = [get_fname(pos_dirpath + str(ii) + '_*.txt') for ii in genre_pos_idx]\n",
    "\n",
    "    # pull the text of the reviews\n",
    "    def get_reviews(dirpath, fnames):\n",
    "        reviews = []\n",
    "        for fname in fnames:\n",
    "            with open(dirpath + fname, 'r') as myfile:\n",
    "                data=myfile.read()\n",
    "            reviews.append(data)\n",
    "        return reviews\n",
    "\n",
    "    genre_neg_reviews = get_reviews(neg_dirpath, genre_neg_fnames)\n",
    "    genre_pos_reviews = get_reviews(pos_dirpath, genre_pos_fnames)\n",
    "    \n",
    "    df_out = pd.DataFrame(data=genre_neg_reviews + genre_pos_reviews)\n",
    "    df_out.columns = ['text']\n",
    "    df_out.title = df_neg['title'].values + df_pos['title'].values\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull drama reviews\n",
    "df_train_drama = get_reviews(\"stanford_train\",\n",
    "                             \"Drama\",\n",
    "                             \"/Users/dfinkel/Downloads/aclImdb/train/\")\n",
    "df_train_drama['class'] = 0\n",
    "\n",
    "# Pull comedy reviews\n",
    "df_train_comedy = get_reviews(\"stanford_train\",\n",
    "                              \"Comedy\",\n",
    "                              \"/Users/dfinkel/Downloads/aclImdb/train/\")\n",
    "df_train_comedy['class'] = 1\n",
    "\n",
    "# build training frame\n",
    "df_train = pd.concat([df_train_drama, df_train_comedy])\n",
    "\n",
    "# shuffle rows\n",
    "df_train = df_train.sample(frac=1).reset_index(drop=True)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19833\n",
      "17448\n"
     ]
    }
   ],
   "source": [
    "print len(df_train)\n",
    "df_train = df_train.drop_duplicates('text')\n",
    "print len(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Tokenize the comments and \n",
    "convert to numerical sequences\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize comments\n",
    "texts = df_train['text'].values\n",
    "\n",
    "# Initialize tokenizer\n",
    "# nb_words tells tokenizer to only keep MAX_NB_WORDS\n",
    "# when texts_to_sequences is applied\n",
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "\n",
    "# encoding text tokens to sequences\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# convert text to numerical arrays\n",
    "sequences = tokenizer.texts_to_sequences(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 75170 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# count unique tokens in corpus\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Shape of data tensor:', (17448, 150))\n",
      "('Shape of label tensor:', (17448, 2))\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Bookkeeping\n",
    "\"\"\"\n",
    "# convert comment labels to categorical tensor\n",
    "labels = df_train['class']\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "\n",
    "# zero-pad comments\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive and negative reviews in traing and validation set \n",
      "[8643. 5316.]\n",
      "[2180. 1309.]\n"
     ]
    }
   ],
   "source": [
    "# randomly sort data into train/test sets\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]\n",
    "\n",
    "print('Number of positive and negative reviews in traing and validation set ')\n",
    "print y_train.sum(axis=0)\n",
    "print y_val.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 400000 word vectors in Glove 6B 100d.\n"
     ]
    }
   ],
   "source": [
    "# import GLOVE word embeddings\n",
    "# see: https://nlp.stanford.edu/projects/glove/\n",
    "GLOVE_DIR = \"/Users/dfinkel/proto_dev/data_science/glove_data/glove_files/\"\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Total %s word vectors in Glove 6B 100d.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create embedding layer \n",
    "using glove data\n",
    "\"\"\"\n",
    "\n",
    "# embedding matrix maps words onto vectors\n",
    "# initialize with uniform random numbers (?)\n",
    "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index \n",
    "        # will be uniform random numbers\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use model from https://arxiv.org/pdf/1404.2188.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMaxPooling(Layer):\n",
    "    \"\"\"\n",
    "    K-max pooling layer that extracts the k-highest activations from a sequence (2nd dimension).\n",
    "    TensorFlow backend.\n",
    "    \"\"\"\n",
    "    def __init__(self, k=1, axis=1, **kwargs):\n",
    "        super(KMaxPooling, self).__init__(**kwargs)\n",
    "        self.input_spec = InputSpec(ndim=3)\n",
    "        self.k = k\n",
    "\n",
    "        assert axis in [1,2],  'expected dimensions (samples, filters, convolved_values),\\\n",
    "                   cannot fold along samples dimension or axis not in list [1,2]'\n",
    "        self.axis = axis\n",
    "\n",
    "        # need to switch the axis with the last elemnet\n",
    "        # to perform transpose for tok k elements since top_k works in last axis\n",
    "        self.transpose_perm = [0,1,2] #default\n",
    "        self.transpose_perm[self.axis] = 2\n",
    "        self.transpose_perm[2] = self.axis\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        input_shape_list = list(input_shape)\n",
    "        input_shape_list[self.axis] = self.k\n",
    "        return tuple(input_shape_list)\n",
    "\n",
    "    def call(self, x):\n",
    "        # swap sequence dimension to get top k elements along axis=1\n",
    "        transposed_for_topk = tf.transpose(x, perm=self.transpose_perm)\n",
    "\n",
    "        # extract top_k, returns two tensors [values, indices]\n",
    "        top_k = tf.nn.top_k(transposed_for_topk, k=self.k, sorted=True, name=None)[0]\n",
    "\n",
    "        # return back to normal dimension but now sequence dimension has only k elements\n",
    "        # performing another transpose will get the tensor back to its original shape\n",
    "        # but will have k as its axis_1 size\n",
    "        transposed_back = tf.transpose(top_k, perm=self.transpose_perm)\n",
    "\n",
    "        return transposed_back\n",
    "\n",
    "\n",
    "class Folding(Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Folding, self).__init__(**kwargs)\n",
    "        self.input_spec = InputSpec(ndim=3)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[1], int(input_shape[2]/2))\n",
    "\n",
    "    def call(self, x):\n",
    "        input_shape = x.get_shape().as_list()\n",
    "\n",
    "        # split the tensor along dimension 2 into dimension_axis_size/2\n",
    "        # which will give us 2 tensors\n",
    "        splits = tf.split(x, num_or_size_splits=int(input_shape[2]/2), axis=2)\n",
    "\n",
    "        # reduce sums of the pair of rows we have split onto\n",
    "        reduce_sums = [tf.reduce_sum(split, axis=2) for split in splits]\n",
    "\n",
    "        # stack them up along the same axis we have reduced\n",
    "        row_reduced = tf.stack(reduce_sums, axis=2)\n",
    "        return row_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 150, 100)          7517100   \n",
      "_________________________________________________________________\n",
      "zero_padding1d_3 (ZeroPaddin (None, 248, 100)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 248, 64)           320064    \n",
      "_________________________________________________________________\n",
      "k_max_pooling_3 (KMaxPooling (None, 5, 64)             0         \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 5, 64)             0         \n",
      "_________________________________________________________________\n",
      "zero_padding1d_4 (ZeroPaddin (None, 53, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 53, 64)            102464    \n",
      "_________________________________________________________________\n",
      "folding_2 (Folding)          (None, 53, 32)            0         \n",
      "_________________________________________________________________\n",
      "k_max_pooling_4 (KMaxPooling (None, 5, 32)             0         \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 5, 32)             0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 322       \n",
      "=================================================================\n",
      "Total params: 7,939,950\n",
      "Trainable params: 7,939,950\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "model based on: https://arxiv.org/pdf/1404.2188.pdf\n",
    "\"\"\"\n",
    "\n",
    "# Initialize model\n",
    "model_1 = Sequential()\n",
    "\n",
    "# Add an embedding layer\n",
    "model_1.add(Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True))\n",
    "\n",
    "# zero bad 49 zeros to \n",
    "# both sides of comment\n",
    "model_1.add(ZeroPadding1D((49,49)))\n",
    "\n",
    "# 1D convolution\n",
    "# 64 channels in output\n",
    "# 50 values in convolution window\n",
    "# zero pad output to perserve size\n",
    "model_1.add(Conv1D(64, 50, padding=\"same\"))\n",
    "\n",
    "# Kmax pool\n",
    "# Return k max values\n",
    "# perserve order\n",
    "model_1.add(KMaxPooling(k=5, axis=1))\n",
    "\n",
    "# Activate w relu\n",
    "model_1.add(Activation(\"relu\"))\n",
    "\n",
    "# zero pad 24 zeros to\n",
    "# both sides of representation\n",
    "model_1.add(ZeroPadding1D((24,24)))\n",
    "\n",
    "# 1D convolution\n",
    "# 64 channels in output\n",
    "# 25 values in convolution window\n",
    "# zero pad output to perserve size\n",
    "model_1.add(Conv1D(64, 25, padding=\"same\"))\n",
    "\n",
    "# Fold output\n",
    "#   - sum 2 channels together\n",
    "#   - halves the channels\n",
    "model_1.add(Folding())\n",
    "\n",
    "# Kmax pool\n",
    "# Return k max values\n",
    "# perserve order\n",
    "model_1.add(KMaxPooling(k=5, axis=1))\n",
    "\n",
    "# Activate w relu\n",
    "model_1.add(Activation(\"relu\"))\n",
    "\n",
    "# Flatten the data\n",
    "model_1.add(Flatten())\n",
    "\n",
    "# Connect the neural net\n",
    "model_1.add(Dense(2, activation=\"softmax\"))\n",
    "\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13959 samples, validate on 3489 samples\n",
      "Epoch 1/10\n",
      "13959/13959 [==============================] - 118s 8ms/step - loss: 0.6984 - acc: 0.6062 - val_loss: 0.6632 - val_acc: 0.6248\n",
      "Epoch 2/10\n",
      "13959/13959 [==============================] - 106s 8ms/step - loss: 0.6665 - acc: 0.6180 - val_loss: 0.7065 - val_acc: 0.6248\n",
      "Epoch 3/10\n",
      "13959/13959 [==============================] - 111s 8ms/step - loss: 0.6652 - acc: 0.6192 - val_loss: 0.6657 - val_acc: 0.6225\n",
      "Epoch 4/10\n",
      "13959/13959 [==============================] - 113s 8ms/step - loss: 0.6520 - acc: 0.6238 - val_loss: 0.6855 - val_acc: 0.5460\n",
      "Epoch 5/10\n",
      "13959/13959 [==============================] - 114s 8ms/step - loss: 0.6071 - acc: 0.6670 - val_loss: 0.7047 - val_acc: 0.5838\n",
      "Epoch 6/10\n",
      "13959/13959 [==============================] - 117s 8ms/step - loss: 0.4993 - acc: 0.7507 - val_loss: 0.7837 - val_acc: 0.5772\n",
      "Epoch 7/10\n",
      "13959/13959 [==============================] - 108s 8ms/step - loss: 0.3417 - acc: 0.8493 - val_loss: 0.9661 - val_acc: 0.5411\n",
      "Epoch 8/10\n",
      "13959/13959 [==============================] - 123s 9ms/step - loss: 0.1981 - acc: 0.9213 - val_loss: 1.2098 - val_acc: 0.5586\n",
      "Epoch 9/10\n",
      "13959/13959 [==============================] - 116s 8ms/step - loss: 0.0944 - acc: 0.9687 - val_loss: 1.5115 - val_acc: 0.5767\n",
      "Epoch 10/10\n",
      "13959/13959 [==============================] - 114s 8ms/step - loss: 0.0340 - acc: 0.9925 - val_loss: 1.7028 - val_acc: 0.5523\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x116fc8c50>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])\n",
    "\n",
    "model_1.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "          nb_epoch=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 3), preprocessor=None, stop_words=None,\n",
       "        st...ty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ = df_train['text'].values\n",
    "Y_ = df_train['class'].values\n",
    "# train classifier\n",
    "# non tf-idf pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(ngram_range=(1, 3))),\n",
    "    ('clf', LogisticRegression(C=0.39810717055349731, class_weight=None)),])\n",
    "pipeline.fit(X_, Y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-935cff78c3ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m# Extract the unigrams, bigrams and trigrams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# used by the trained classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0munigrams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbigrams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrigrams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-62-935cff78c3ac>\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(pipeline)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# vect = pipeline.steps[0][1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# clf = pipeline.steps[2][1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mtopn_class1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn_class2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmost_informative_feature_for_binary_classification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mclass1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtopn_class1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'weight'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'word'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "def most_informative_feature_for_binary_classification(pipeline, n=10):\n",
    "    class_labels = pipeline.named_steps['clf'].classes_\n",
    "    if 'feats' in pipeline.named_steps.keys():\n",
    "        feature_names = pipeline.named_steps['feats'].get_feature_names()\n",
    "    elif 'vect' in pipeline.named_steps.keys():\n",
    "        feature_names = pipeline.named_steps['vect'].get_feature_names()\n",
    "    topn_class1 = sorted(zip(pipeline.named_steps['clf'].coef_[0], feature_names))[:n]\n",
    "    topn_class2 = sorted(zip(pipeline.named_steps['clf'].coef_[0], feature_names))[-n:]\n",
    "\n",
    "#     for coef, feat in topn_class1:\n",
    "#         print class_labels[0], coef, feat\n",
    "\n",
    "#     print\n",
    "\n",
    "#     for coef, feat in reversed(topn_class2):\n",
    "#         print class_labels[1], coef, feat\n",
    "    return topn_class1, topn_class2\n",
    "\n",
    "def extract_features(pipeline):\n",
    "    \"\"\"\n",
    "    extract the informative features\n",
    "    from the classifier\n",
    "    \"\"\"\n",
    "    # vect = pipeline.steps[0][1]\n",
    "    # clf = pipeline.steps[2][1]\n",
    "    topn_class1, topn_class2 = most_informative_feature_for_binary_classification(pipeline, n=200000)\n",
    "\n",
    "    class1 = pd.DataFrame(data=topn_class1, columns=(('weight', 'word')))\n",
    "    class2 = pd.DataFrame(data=topn_class2, columns=(('weight', 'word')))\n",
    "\n",
    "    class1['ngram_size'] = class1['word'].apply(meas_length)\n",
    "    class2['ngram_size'] = class2['word'].apply(meas_length)\n",
    "\n",
    "    # name columns\n",
    "    class1.columns = ['Weight', 'word', 'ngram_size']\n",
    "    class2.columns = ['Weight', 'word', 'ngram_size']\n",
    "\n",
    "    # break out unigrams, bigrams and trigrams\n",
    "    unigrams = pd.concat([class1.loc[class1['ngram_size'] == 1], class2.loc[class2['ngram_size'] == 1]])\n",
    "    bigrams = pd.concat([class1.loc[class1['ngram_size'] == 2], class2.loc[class2['ngram_size'] == 2]])\n",
    "    trigrams = pd.concat([class1.loc[class1['ngram_size'] == 3], class2.loc[class2['ngram_size'] == 3]])\n",
    "    return unigrams, bigrams, trigrams\n",
    "\n",
    "# Extract the unigrams, bigrams and trigrams\n",
    "# used by the trained classifier\n",
    "unigrams, bigrams, trigrams = extract_features(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.predict(['bruce', 'cusack'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17448 6625 10823\n"
     ]
    }
   ],
   "source": [
    "print len(Y_), np.sum(Y_), len(Y_) - np.sum(Y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = pipeline.named_steps['clf'].classes_\n",
    "if 'feats' in pipeline.named_steps.keys():\n",
    "    feature_names = pipeline.named_steps['feats'].get_feature_names()\n",
    "elif 'vect' in pipeline.named_steps.keys():\n",
    "    feature_names = pipeline.named_steps['vect'].get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "topn_class1 = sorted(zip(pipeline.named_steps['clf'].coef_[0], feature_names))[:n]\n",
    "topn_class2 = sorted(zip(pipeline.named_steps['clf'].coef_[0], feature_names))[-n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-0.39825643274804623, u'cool'),\n",
       " (-0.39021804290406786, u'bruce'),\n",
       " (-0.3844036176860867, u'drama'),\n",
       " (-0.3609975417327202, u'score'),\n",
       " (-0.3605436219098733, u'there were'),\n",
       " (-0.3594451751334926, u'jerry'),\n",
       " (-0.3520874980464845, u'negative'),\n",
       " (-0.32477966303041717, u'johnny'),\n",
       " (-0.31719365946169253, u'william'),\n",
       " (-0.31371268870647956, u'camp'),\n",
       " (-0.3065744449726966, u'carrey'),\n",
       " (-0.3002654278718152, u'apart'),\n",
       " (-0.30005766001178674, u'sandler'),\n",
       " (-0.2954877416410513, u'70'),\n",
       " (-0.29204944550893064, u'minor'),\n",
       " (-0.28890971956636874, u'what he'),\n",
       " (-0.2885285049002655, u'very good'),\n",
       " (-0.2879559253586889, u'murders'),\n",
       " (-0.28771482200647164, u'editing'),\n",
       " (-0.28734658415890824, u'language'),\n",
       " (-0.28513352402933645, u'yourself'),\n",
       " (-0.2837746062065589, u'is one of'),\n",
       " (-0.28258459481739306, u'human'),\n",
       " (-0.282387640244754, u'it can'),\n",
       " (-0.28180395918058765, u'not worth'),\n",
       " (-0.2795779830834692, u'fantastic'),\n",
       " (-0.2782520972184833, u'comedies'),\n",
       " (-0.2743665040128819, u'brosnan'),\n",
       " (-0.2743163353336603, u'along'),\n",
       " (-0.27287746890152553, u'dahmer'),\n",
       " (-0.27195071924801456, u'oscars'),\n",
       " (-0.2717201382629541, u'movie when'),\n",
       " (-0.2707613809397806, u'on dvd'),\n",
       " (-0.2696492461375536, u'we see'),\n",
       " (-0.26744574338002647, u'branagh'),\n",
       " (-0.26669180987645863, u'jim'),\n",
       " (-0.26560348106390613, u'ever seen'),\n",
       " (-0.2643650383743202, u'lawyer'),\n",
       " (-0.2630195933947609, u'shakespeare'),\n",
       " (-0.2605986608334245, u'elvira'),\n",
       " (-0.2583683789645916, u'eye'),\n",
       " (-0.2561383212018168, u'there no'),\n",
       " (-0.2560121029038225, u'working'),\n",
       " (-0.2551930850370387, u'virtually'),\n",
       " (-0.25459002827800337, u'victoria'),\n",
       " (-0.2544680766934954, u'this movie and'),\n",
       " (-0.2531279590663254, u'sex'),\n",
       " (-0.25228555856638274, u'to be the'),\n",
       " (-0.25091774335672573, u'askey'),\n",
       " (-0.24882938519824696, u'it made'),\n",
       " (-0.24770950604996875, u'prince'),\n",
       " (-0.24769584470138395, u'grim'),\n",
       " (-0.24674715403172506, u'the women'),\n",
       " (-0.24645087892291584, u'are really'),\n",
       " (-0.2442424324735007, u'me br'),\n",
       " (-0.2442424324735007, u'me br br'),\n",
       " (-0.24391651496730696, u'japanese'),\n",
       " (-0.24323804846921296, u'all it'),\n",
       " (-0.2421707603510647, u'even the'),\n",
       " (-0.24149212000694353, u'shooting'),\n",
       " (-0.240771676014254, u'caught'),\n",
       " (-0.24018569203539447, u'the dark'),\n",
       " (-0.23981847132518036, u'especially'),\n",
       " (-0.23803561282814564, u'started'),\n",
       " (-0.23721355020178134, u'pity'),\n",
       " (-0.23643902955018836, u'dubbing'),\n",
       " (-0.23572924897275205, u'vs'),\n",
       " (-0.23542352408625752, u'jones'),\n",
       " (-0.23447270796181433, u'aliens'),\n",
       " (-0.2343905547067797, u'oh'),\n",
       " (-0.23433985646347372, u'eva'),\n",
       " (-0.23359223365607268, u'edited'),\n",
       " (-0.23197845517149104, u'liking'),\n",
       " (-0.23057777289833623, u'situations'),\n",
       " (-0.22929202749886057, u'weak'),\n",
       " (-0.22919293707103439, u'lloyd'),\n",
       " (-0.2291920162594809, u'writing'),\n",
       " (-0.22891575300740605, u'thing that'),\n",
       " (-0.2286765687982966, u'as good'),\n",
       " (-0.22798527146724343, u'br br was'),\n",
       " (-0.22798527146724343, u'br was'),\n",
       " (-0.22770541674001668, u'but this is'),\n",
       " (-0.2270771161577142, u'an attempt'),\n",
       " (-0.22676746063054762, u'you could'),\n",
       " (-0.22653657640743846, u'field'),\n",
       " (-0.2260971197660801, u'french'),\n",
       " (-0.22603172076208433, u'noir'),\n",
       " (-0.22580614497745968, u'anthony'),\n",
       " (-0.22534401768837087, u'and the characters'),\n",
       " (-0.22455353842802947, u'gandhi'),\n",
       " (-0.22421537432769864, u'you don'),\n",
       " (-0.22316296218754453, u'bakshi'),\n",
       " (-0.2229253436186626, u'fast'),\n",
       " (-0.22259463100725385, u'chess'),\n",
       " (-0.22245776758160546, u'about it'),\n",
       " (-0.22214570402306721, u'logic'),\n",
       " (-0.22203024306312105, u'movie don'),\n",
       " (-0.2213705214848832, u'stiller'),\n",
       " (-0.22133928225416327, u'under'),\n",
       " (-0.2213307076827745, u'danny')]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topn_class1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.23910153207981566, u'decided'),\n",
       " (0.2393177980793017, u'her in'),\n",
       " (0.23993820465989013, u'top notch'),\n",
       " (0.23997217514598002, u'reed'),\n",
       " (0.2406227094961414, u'see this'),\n",
       " (0.2416905939566745, u'she has'),\n",
       " (0.24233253257266898, u'miyazaki'),\n",
       " (0.24285631064043098, u'really great'),\n",
       " (0.2432265892297749, u'where they'),\n",
       " (0.24366884113405304, u'blah'),\n",
       " (0.2442749522910675, u'zizek'),\n",
       " (0.24454422018457087, u'kiss'),\n",
       " (0.2445999719138599, u'summer'),\n",
       " (0.24476688538101588, u'experience'),\n",
       " (0.24484202407764408, u'particularly'),\n",
       " (0.24527655956522282, u'gielgud'),\n",
       " (0.2453002223911645, u'pure'),\n",
       " (0.2453594455310116, u'robert'),\n",
       " (0.24558650961868195, u'marie'),\n",
       " (0.24566471797539957, u'it bad'),\n",
       " (0.24704094942018745, u'but it'),\n",
       " (0.24744296789893255, u'fbi'),\n",
       " (0.24922163723206053, u'but this was'),\n",
       " (0.24939849032141448, u'cute'),\n",
       " (0.2502926632068763, u'the direction'),\n",
       " (0.2511370064504894, u'darius'),\n",
       " (0.2513535693741552, u'din'),\n",
       " (0.25156971212101426, u'underground'),\n",
       " (0.2520003835842697, u'the plot'),\n",
       " (0.2525730247531832, u'crap'),\n",
       " (0.25274521380802384, u'garbage'),\n",
       " (0.2532630496067412, u'drew'),\n",
       " (0.25352975676612305, u'the future'),\n",
       " (0.2539396078795351, u'flop'),\n",
       " (0.25429968671992587, u'in any'),\n",
       " (0.2545964421499946, u'routine'),\n",
       " (0.2565040341265307, u'of great'),\n",
       " (0.25726541773203987, u'included'),\n",
       " (0.2573820373829815, u'on her'),\n",
       " (0.25948313499418646, u'pokemon'),\n",
       " (0.2600071825176157, u'witherspoon'),\n",
       " (0.2600746146379352, u'campbell'),\n",
       " (0.2610139470844134, u'yokai'),\n",
       " (0.2629097893328675, u'todd'),\n",
       " (0.2631629075861161, u'ruth'),\n",
       " (0.26382491716410045, u'parrot'),\n",
       " (0.26392818714054483, u'police'),\n",
       " (0.2647690428353274, u'wendigo'),\n",
       " (0.2667860807481728, u'london'),\n",
       " (0.27016552253699844, u'cusack'),\n",
       " (0.2726028276762625, u'conflict'),\n",
       " (0.2738726697577138, u'you to'),\n",
       " (0.27415750265083666, u'cast the'),\n",
       " (0.27421989888755766, u'his life'),\n",
       " (0.27438360215233115, u'goldsworthy'),\n",
       " (0.2753585182955244, u'story is'),\n",
       " (0.27555215323925664, u'jessica'),\n",
       " (0.2756094074077955, u'of the greatest'),\n",
       " (0.27610928020116804, u'barrymore'),\n",
       " (0.27695906897272754, u'fred'),\n",
       " (0.2774025853042039, u'of life'),\n",
       " (0.277955187111814, u'elvis'),\n",
       " (0.27822264080675485, u'what she'),\n",
       " (0.2789695793638113, u'doc'),\n",
       " (0.284233853406406, u'after the'),\n",
       " (0.2855262917144149, u'woody'),\n",
       " (0.2863921006527553, u'of movie'),\n",
       " (0.2866162820194059, u'many times'),\n",
       " (0.28791732563198763, u'western'),\n",
       " (0.2908818432709708, u'60'),\n",
       " (0.29326720298540027, u'sick'),\n",
       " (0.2994905892487924, u'stories'),\n",
       " (0.3022213351038652, u'zorro'),\n",
       " (0.30355020622244044, u'been more'),\n",
       " (0.30376889216533576, u'rap'),\n",
       " (0.3043353249890952, u'lee'),\n",
       " (0.304482359926236, u'the screen'),\n",
       " (0.30616902683283764, u'ned'),\n",
       " (0.30632292837749825, u'pleasure'),\n",
       " (0.30732575563903636, u'dog'),\n",
       " (0.30770972833771415, u'abc'),\n",
       " (0.30859412291836796, u'the 60'),\n",
       " (0.31572114648660105, u'hat'),\n",
       " (0.317298061145982, u'ashley'),\n",
       " (0.3201106469984887, u'culture'),\n",
       " (0.3207708482492872, u'joe don'),\n",
       " (0.3224278233118832, u'tape'),\n",
       " (0.32401384115632476, u'waters'),\n",
       " (0.33238242723473754, u'remake'),\n",
       " (0.3464714119535743, u'mst3k'),\n",
       " (0.34773156464061333, u'or the'),\n",
       " (0.35287418297808587, u'jackie'),\n",
       " (0.35565561712804933, u'daughters'),\n",
       " (0.3705207309418046, u'snowman'),\n",
       " (0.38279371261913336, u'changes'),\n",
       " (0.39541914546290235, u'bugs'),\n",
       " (0.4125774795039569, u'columbo'),\n",
       " (0.42078973756396837, u'verhoeven'),\n",
       " (0.4401193015437623, u'steve'),\n",
       " (0.5557932110223566, u'paulie')]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topn_class2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[\"This is without a shadow of a doubt the absolute worst movie Steven Seagal has ever made. And that says a lot. Don't get fooled by the rating, it's way too good. This abomination hadn't even been worthy of a 0/10 rating, if such a thing existed. <br /><br />- Absolutely no plot <br /><br />- Worst action scenes ever, and there aren't too many of them either <br /><br />- Seagal doesn't do anything himself, including the fighting, talking (lots of dubbing), and so on. As always. <br /><br />- Seagal is fat, lazy and couldn't care less about this movie. Something which is very obvious all the way through<br /><br />Take all the other garbage DTV movies Seagal has made, multiply them with each other, multiply this with a thousand billions, and all the badness you then get won't even describe 1 % of this absolute crapfest.\",\n",
       "        0],\n",
       "       [\"I was very displeased with this move. Everything was terrible from the start. The comedy was unhumorous, the action overdone, the songs unmelodious. Even the storyline was weightless. From a writer who has written successful scripts like Guru and Dhoom, I had high expectations. The actors worked way too hard and did not help the film at all. Of course, Kareena rocked the screen in a bikini but for two seconds. I think Hindi stunt directors should research how action movies are done. They tend to exaggerate way too much. In Chinese films, this style works because that is their signature piece. But, Hindi cinema's signature are the songs. A good action movie should last no more than two hours and cannot look unrealistic. But, in the future, I'm sure these action movies will get much sharper. Also to be noted: Comedy and action films do not mix unless done properly. Good Luck next time.\",\n",
       "        0],\n",
       "       [\"for all the subtle charms this student film may contain, was anyone else bored to death waiting WENDINGO to show his paper macho face??<br /><br />the anti-climax pretty much ruined any sort of momentum we had speed actioned to develop.<br /><br />don't get me wrong, i'm all into exploring America's dark underbelly, but this is a turd-a-flamb\\xc3\\xa9 that gets a nod to watchable only for the fact that p.clarkson looks hot taking it.<br /><br />sadly, from a guy from wings.<br /><br />the best 2 minutes the film has to offer.<br /><br />if you felt like ripping off DELIVERANCE, you could do better.\",\n",
       "        1],\n",
       "       [\"I've seen the original non-dubbed German version and I was surprised how bad this movie actually is. Thinking I had seen my share of bad movies like Ghoulies 2, Rabid Grannies, Zombie Lake and such, nothing could've prepared me for this! It really was a pain to sit through this flick, as there's no plot, no good acting and even the special effects aren't convincing, especially the so-called zombies, wearing nothing more than white make-up and their old clothes, so their good set wouldn't be ruined by ketchup and marmalade stains. <br /><br />If you really want to waste 90 minutes of your life, then watch it, for all the others, don't do it, because you WILL regret it!\",\n",
       "        0],\n",
       "       [\"So i consider myself pretty big into the anime scene, with very few shows i simply WILL NOT WATCH.<br /><br />this show, however, i would recommend to anyone.<br /><br />Quite possibly the most Original series to date, it;s got just about everything i could ask for. A side story, so to speak, about an unconditional love that will NOT be admitted to, a very blatant comedy, and a very well put together voice acting cast (both Japanese and American translation).<br /><br />If not for the terribly funny aspect to it, it would be, just another anime.<br /><br />More or less, as i have noticed, a 'love it or hate it', very few people i have seen introduced to this series will end up with a distaste for it.<br /><br />Original to the core, with everything you could ask for in an afternoon, bet the house on this series. I'm ready to ASSURE you that you will enjoy it.\",\n",
       "        1]], dtype=object)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
